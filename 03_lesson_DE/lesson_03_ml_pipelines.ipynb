{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipelines\n",
    "\n",
    "* Advantages of Machine Learning Pipelines\n",
    "* Scikit-learn Pipeline\n",
    "* Scikit-learn Feature Union\n",
    "* Pipelines and Grid Search\n",
    "* Case Study\n",
    "\n",
    "**CASE STUDY:**\n",
    "#### Corporate Messaging Case Study\n",
    "This corporate message data is from one of the free datasets provided on the [Figure Eight Platform](https://www.figure-eight.com/data-for-everyone/), licensed under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet'])\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv('corporate_messaging.csv', encoding='latin-1')\n",
    "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
    "    X = df.text.values\n",
    "    y = df.category.values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Function to clean text\n",
    "    \n",
    "    - Replaces URLs with \"urlplaceholder\"\n",
    "    - Tokenizes\n",
    "    - Lemmatizes\n",
    "    - Removes extra whitespace\n",
    "    - Transforms to lowercase\n",
    "    \n",
    "    :param text (str): string data\n",
    "    \n",
    "    :return clean_tokens (lst): list of cleaned tokens\n",
    "    \"\"\"\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|\\[$-_@.&+]|[!*\\(\\),\\]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    \n",
    "    detected_urls = re.findall(url_regex, text)    \n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_tokens = [lemmatizer.lemmatize(token).lower().strip() for token in tokens]\n",
    "    clean_tokens = [lemmatizer.lemmatize(token, pos='v') for token in clean_tokens]\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "def transformer(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "    #2\n",
    "    vect = CountVectorizer(tokenizer=tokenize)\n",
    "    tfidf = TfidfTransformer()\n",
    "    \n",
    "    X_train_count = vect.fit_transform(X_train)\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train_count)\n",
    "    #3\n",
    "    X_test_count = vect.transform(X_test)\n",
    "    X_test_tfidf = tfidf.transform(X_test_count)\n",
    "    \n",
    "    return X_train_tfidf, y_train, X_test_tfidf, y_test\n",
    "\n",
    "\n",
    "def trainer(X_train_tfidf,\n",
    "            y_train,\n",
    "            X_test_tfidf,\n",
    "            clf):\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "    y_pred = clf.predict(X_test_tfidf)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def display_results(y_test, y_pred):\n",
    "    labels = np.array(list(set(y_test)), dtype='object')\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    display(pd.DataFrame(confusion_mat,\n",
    "            columns=[lab + \"_true\" for lab in labels],\n",
    "            index=[lab + \"_pred\" for lab in labels]))\n",
    "    print(\"Accuracy:\", round(accuracy, 4))\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def main(clf):\n",
    "    #1\n",
    "    X, y = load_data()\n",
    "    #2, 3\n",
    "    X_train_tfidf, y_train, X_test_tfidf, y_test = transformer(X, y)\n",
    "    y_pred = trainer(X_train_tfidf, y_train, X_test_tfidf, clf)\n",
    "    #4\n",
    "    display_results(y_test, y_pred)\n",
    "    \n",
    "    return None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline: SKLearn function\n",
    "Instead of writing this in the form of unique functions, we can use SKLearn's `Pipeline()` method\n",
    "\n",
    "Below, you'll find a simple example of a machine learning workflow where we generate features from text data using count vectorizer and tf-idf transformer, and then fit it to a random forest classifier. Before we get into using pipelines, let's first use this example to go over some scikit-learn terminology.\n",
    "\n",
    "* **Estimator:** An estimator is any object that learns from data, whether it's a classification, regression, or clustering algorithm, or a transformer that extracts or filters useful features from raw data. Since estimators learn from data, they each must have a fit method that takes a dataset. In the example below, the CountVectorizer, TfidfTransformer, and RandomForestClassifier are all estimators, and each have a fit method.\n",
    "\n",
    "* **Transform:** A transformer is a specific type of estimator that has a fit method to learn from training data, and then a transform method to apply a transformation model to new data. These transformations can include cleaning, reducing, expanding, or generating features. In the example below, CountVectorizer and TfidfTransformer are transformers.\n",
    "\n",
    "* **Predictor:** A predictor is a specific type of estimator that has a predict method to predict on test data based on a supervised learning algorithm, and has a fit method to train the model on training data. The final estimator, RandomForestClassifier, in the example below is a predictor.\n",
    "\n",
    "In machine learning tasks, it's pretty common to have a very specific sequence of transformers to fit to data before applying a final estimator, such as this classifier. And normally, we'd have to initialize all the estimators, fit and transform the training data for each of the transformers, and then fit to the final estimator. Next, we'd have to call transform for each transformer again to the test data, and finally call predict on the final estimator.\n",
    "\n",
    "**NOTE** Every step of the `Pipeline()` has to be a transformer *EXCEPT* for the last step\n",
    "\n",
    "#### Without `Pipeline()`:\n",
    "```python\n",
    "    vect = CountVectorizer()\n",
    "    tfidf = TfidfTransformer()\n",
    "    clf = RandomForestClassifier()\n",
    "\n",
    "    # train classifier\n",
    "    X_train_counts = vect.fit_transform(X_train)\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # predict on test data\n",
    "    X_test_counts = vect.transform(X_test)\n",
    "    X_test_tfidf = tfidf.transform(X_test_counts)\n",
    "    y_pred = clf.predict(X_test_tfidf)\n",
    "```\n",
    "\n",
    "#### With `Pipeline()`:\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "\n",
    "# build pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier(random_state=0))\n",
    "])\n",
    "# train classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "# predict on test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "Now, by fitting our pipeline to the training data, we're accomplishing exactly what we would by fitting and transforming each of these steps to our training data one by one. Similarly, when we call `predict` on our pipeline to our test data, we're accomplishing what we would by calling `transform` on each of our transformer objects to our test data and then calling `predict` on our final estimator. Not only does this make our code much shorter and simpler, it has other great advantages, which we'll cover in the next video.\n",
    "\n",
    "Note that every step of this pipeline has to be a transformer, except for the last step, which can be of an estimator type. Pipeline takes on all the methods of whatever the last estimator in its sequence is. For example, here, since the final estimator of our pipeline is a classifier, the pipeline object can be used as a classifier, taking on the `fit` and `predict` methods of its last step. Alternatively, if the last estimator was a transformer, then pipeline would be a transformer.\n",
    "\n",
    "### Pipeline: Advantages\n",
    "#### 1. Simplicity and Convencience\n",
    "* **Automates repetitive steps** - Chaining all of your steps into one estimator allows you to fit and predict on all steps of your sequence automatically with one call. It handles smaller steps for you, so you can focus on implementing higher level changes swiftly and efficiently.\n",
    "\n",
    "* **Easily understandable workflow** - Not only does this make your code more concise, it also makes your workflow much easier to understand and modify. Without Pipeline, your model can easily turn into messy spaghetti code from all the adjustments and experimentation required to improve your model.\n",
    "\n",
    "* **Reduces mental workload** - Because Pipeline automates the intermediate actions required to execute each step, it reduces the mental burden of having to keep track of all your data transformations. Using Pipeline may require some extra work at the beginning of your modeling process, but it prevents a lot of headaches later on.\n",
    "\n",
    "#### 2. Optimizing Entire Workflow\n",
    "* **Grid Search:** Method that automates the process of testing different hyper parameters to optimize a model.\n",
    "* By running grid search on your pipeline, you're able to optimize your entire workflow, including data transformation and modeling steps. This accounts for any interactions among the steps that may affect the final metrics.\n",
    "* Without grid search, tuning these parameters can be painfully slow, incomplete, and messy.\n",
    "\n",
    "#### 3. Preventing Data leakage\n",
    "* Using Pipeline, all transformations for data preparation and feature extractions occur within each fold of the cross validation process.\n",
    "* This prevents common mistakes where you’d allow your training process to be influenced by your test data - for example, if you used the entire training dataset to normalize or extract features from your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline and Feature Unions\n",
    "* **Feature Union:** Feature union is a class in scikit-learn’s Pipeline module that allows us to perform steps in parallel and take the union of their results for the next step.\n",
    "* A pipeline performs a list of steps in a linear sequence, while a feature union performs a list of steps in parallel and then combines their results.\n",
    "* In more complex workflows, multiple feature unions are often used within pipelines, and multiple pipelines are used within feature unions.\n",
    "<img src='ml_feat_un_0.png'>\n",
    "\n",
    "Sometimes, you won't have all the data transformation steps you need in scikit-learn's library, which is why it is possible to actually create your own custom transformers. Keep in mind that `TextLengthExtractor` is a custom transformer that is already built in a separate file and imported for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
