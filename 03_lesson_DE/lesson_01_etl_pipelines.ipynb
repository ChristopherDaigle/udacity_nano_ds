{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipelines: ETL vs ELT\n",
    "Data pipeline is a generic term for moving data from one place to another. For example, it could be moving data from one server to another server.\n",
    "\n",
    "## ETL\n",
    "An [ETL pipeline](https://en.wikipedia.org/wiki/Extract,_transform,_load) is a specific kind of data pipeline and very common. ETL stands for Extract, Transform, Load. Imagine that you have a database containing web [log data](https://en.wikipedia.org/wiki/Log_file). Each entry contains the IP address of a user, a timestamp, and the link that the user clicked.\n",
    "\n",
    "What if your company wanted to run an analysis of links clicked by city and by day? You would need another data set that maps IP address to a city, and you would also need to extract the day from the timestamp. With an ETL pipeline, you could run code once per day that would extract the previous day's log data, map IP address to city, aggregate link clicks by city, and then load these results into a new database. That way, a data analyst or scientist would have access to a table of log data by city and day. That is more convenient than always having to run the same complex data transformations on the raw web log data.\n",
    "\n",
    "Before cloud computing, businesses stored their data on large, expensive, private servers. Running queries on large data sets, like raw web log data, could be expensive both economically and in terms of time. But data analysts might need to query a database multiple times even in the same day; hence, pre-aggregating the data with an ETL pipeline makes sense.\n",
    "\n",
    "## ELT\n",
    "ELT (Extract, Load, Transform) pipelines have gained traction since the advent of cloud computing. Cloud computing has lowered the cost of storing data and running queries on large, raw data sets. Many of these cloud services, like [Amazon Redshift](https://aws.amazon.com/redshift/), [Google BigQuery](https://cloud.google.com/bigquery/), or [IBM Db2](https://www.ibm.com/cloud/db2-warehouse-on-cloud) can be queried using SQL or a SQL-like language. With these tools, the data gets extracted, then loaded directly, and finally transformed at the end of the pipeline.\n",
    "\n",
    "However, ETL pipelines are still used even with these cloud tools. Oftentimes, it still makes sense to run ETL pipelines and store data in a more readable or intuitive format. This can help data analysts and scientists work more efficiently as well as help an organization become more data driven."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline of the Lesson\n",
    "1. Extract data from different sources such as:\n",
    "\n",
    ">* csv files\n",
    ">* json files\n",
    ">* APIs\n",
    "\n",
    "2. Transform data\n",
    "\n",
    ">* combining data from different sources\n",
    ">* data cleaning\n",
    ">* data types\n",
    ">* parsing dates\n",
    ">* file encodings\n",
    ">* missing data\n",
    ">* duplicate data\n",
    ">* dummy variables\n",
    ">* remove outliers\n",
    ">* scaling features\n",
    ">* engineering features\n",
    "\n",
    "3. Load\n",
    "\n",
    ">* send the transformed data to a database\n",
    "\n",
    "4. ETL Pipeline\n",
    "\n",
    ">* code an ETL pipeline\n",
    "\n",
    "This lesson contains many Jupyter notebook exercises where you can practice the different parts of an ETL pipeline. Some of the exercises are challenging, but they also contain hints to help you get through them. You'll notice that the \"transformation\" section is relatively long. You'll oftentimes hear data scientists say that cleaning and transforming data is how they spend a majority of their time. This lesson reflects that reality.\n",
    "\n",
    "#### Big Data Courses at Udacity\n",
    "\"Big Data\" gets a lot of buzz these days, and it is definitely an important part of a data engineer's and, sometimes, a data scientists's work. With \"Big Data\", you need special tools that can work on distributed computer systems.\n",
    "\n",
    "This ETL course focuses on the practical fundamentals of ETL. Hence, you'll be working with a local data set so that you do not need to worry about learning a new tool. Udacity has other courses where the primary focus is on tools used for distributed data sets.\n",
    "\n",
    "Here are links to other big data courses at Udacity:\n",
    "* [Intro to Hadoop and MapReduce](https://www.udacity.com/course/intro-to-hadoop-and-mapreduce--ud617)\n",
    "* [Deploying a Hadoop Cluster](https://www.udacity.com/course/deploying-a-hadoop-cluster--ud1000)\n",
    "* [Real-time Analytics with Apache Storm](https://www.udacity.com/course/real-time-analytics-with-apache-storm--ud381)\n",
    "* [Big Data Analytics in Health Care](https://www.udacity.com/course/big-data-analytics-in-healthcare--ud758)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World Bank Data\n",
    "In the next section, you'll find a series of exercises. These are relatively brief and focus on extracting, or in other words, reading in data from different sources. The goal is to familiarize yourself with different types of files and see how the same data can be formatted in different ways. This lesson assumes you have experience with pandas and basic programming skills.\n",
    "[VIDEO](https://youtu.be/lNPzOLzZVbw)\n",
    "\n",
    "This lesson uses data from the World Bank. The data comes from two sources:\n",
    "1. [World Bank Indicator Data](https://data.worldbank.org/indicator) - This data contains socio-economic indicators for countries around the world. A few example indicators include population, arable land, and central government debt.\n",
    "\n",
    "2. [World Bank Project Data](https://datacatalog.worldbank.org/dataset/world-bank-projects-operations) - This data set contains information about World Bank project lending since 1947.\n",
    "Both of these data sets are available in different formats including as a csv file, json, or xml. You can download the csv directly or you can use the World Bank APIs to extract data from the World Bank's servers. You'll be doing both in this lesson.\n",
    "\n",
    "The end goal is to clean these data sets and bring them together into one table. As you'll see, it's not as easy as one might hope. By the end of the lesson, you'll have written an ETL pipeline to extract, transform, and load this data into a new database.\n",
    "\n",
    "The goal of the lesson is to combine these data sets together so that you can run a linear regression model predicting World Bank Project total costs. You will not actually build the model; instead, you will get the data ready so that a data analyst or data scientist could more easily build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
