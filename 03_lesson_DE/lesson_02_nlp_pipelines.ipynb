{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Pipelines\n",
    "\n",
    "In this lesson, you'll be introduced to some of the steps involved in a NLP pipeline:\n",
    "\n",
    "1. Text Processing\n",
    "\n",
    ">* Cleaning\n",
    ">* Normalization\n",
    ">* Tokenization\n",
    ">* Stop Word Removal\n",
    ">* Part of Speech Tagging\n",
    ">* Named Entity Recognition\n",
    ">* Stemming and Lemmatization\n",
    "\n",
    "2. Feature Extraction\n",
    "\n",
    ">* Bag of Words\n",
    ">* TF-IDF\n",
    ">* Word Embeddings\n",
    "\n",
    "3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How NLP Pipelines Work\n",
    "The 3 stages of an NLP pipeline are: Text Processing -> Feature Extraction -> Modeling.\n",
    "\n",
    "1. **Text Processing:** Take raw input text, clean it, normalize it, and convert it into a form that is suitable for feature extraction.\n",
    "\n",
    "2. **Feature Extraction:** Extract and produce feature representations that are appropriate for the type of NLP task you are trying to accomplish and the type of model you are planning to use.\n",
    "\n",
    "3. **Modeling:** Design a statistical or machine learning model, fit its parameters to training data, use an optimization procedure, and then use it to make predictions about unseen data.\n",
    "\n",
    "This process isn't always linear and may require additional steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Text Processing\n",
    "The first chunk of this lesson will explore the steps involved in text processing, the first stage of the NLP pipeline.\n",
    "\n",
    "* **Extracting plain text:** Textual data can come from a wide variety of sources: the web, PDFs, word documents, speech recognition systems, book scans, etc. Your goal is to extract plain text that is free of any source specific markup or constructs that are not relevant to your task.\n",
    "* **Reducing complexity:** Some features of our language like capitalization, punctuation, and common words such as a, of, and the, often help provide structure, but don't add much meaning. Sometimes it's best to remove them if that helps reduce the complexity of the procedures you want to apply later.\n",
    "\n",
    "In this lesson...\n",
    "> You'll prepare text data from different sources with the following text processing steps:\n",
    "\n",
    "1. **Cleaning** to remove irrelevant items, such as HTML tags\n",
    "2. **Normalizing** by converting to all lowercase and removing punctuation\n",
    "3. Splitting text into words or **tokens**\n",
    "4. Removing words that are too common, also known as **stop words**\n",
    "5. Identifying different **parts of speech** and **named entities**\n",
    "6. Converting words into their dictionary forms, using **stemming and lemmatization**\n",
    "\n",
    "After performing these steps, your text will capture the essence of what was being conveyed in a form that is easier to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing: Cleaning\n",
    "Let's walk through an example of cleaning text data from a popular source - the web. You'll be introduced to helpful tools in working with this data, including the `requests` library, **regular expressions**, and `Beautiful Soup`.\n",
    "\n",
    "**Documentation for Python Libraries:**\n",
    "* [Requests](http://docs.python-requests.org/en/master/user/quickstart/#make-a-request)\n",
    "* [Regular Expressions](https://docs.python.org/3/library/re.html)\n",
    "* [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMPLE:\n",
    "```python\n",
    "# import statements\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# fetch web page\n",
    "r = requests.get(\"https://www.udacity.com/courses/all\")\n",
    "r\n",
    ">>> <Response [200]>\n",
    "\n",
    "soup = BeautifulSoup(markup=r.text, features=\"lxml\")\n",
    "soup\n",
    ">>> <!DOCTYPE html>\n",
    ">>> <html lang=\"en-US\"><head>\n",
    ">>> <meta charset=\"utf-8\"/>\n",
    ">>> <script class=\"ng-star-inserted\" ...\n",
    ">>> ...\n",
    ">>> &q;type&q;:&q;category&q;,&q;matchCriteria&q;:{&q;withKey&q;\n",
    ">>> :&q;VR development&q;}}]}]}</script></body></html>\n",
    "        \n",
    "# Find all course summaries\n",
    "summaries = soup.find_all(\"div\", {\"class\": \"course-summary-card\"})\n",
    "print('Number of Courses:', len(summaries))\n",
    ">>> Number of Courses: 250\n",
    "\n",
    "# print the first summary in summaries\n",
    "print(summaries[0].prettify())\n",
    ">>> <div _ngcontent-sc154=\"\" class=\"...\">\n",
    ">>>  <ir-catalog-card _ngcontent-sc154=\"\" _nghost-sc157=\"\">\n",
    ">>>   <div _ngcontent-sc157=\"\" class=\"card-wrapper is-collapsed\">\n",
    ">>> ...\n",
    ">>> </div>\n",
    "\n",
    "# Extract course title\n",
    "summaries[0].select_one(\"h3\").get_text()\n",
    ">>> 'Applying Data Science to Product Management'\n",
    "\n",
    "# Extract school\n",
    "summaries[0].select_one(\"h4\").get_text().strip()\n",
    ">>> 'School of Business'\n",
    "\n",
    "# append name and school of each summary to courses list\n",
    "courses = []\n",
    "for summary in summaries:\n",
    "    name = summary.select_one(\"h3\").get_text()\n",
    "    school = summary.select_one(\"h4\").get_text().strip()\n",
    "    courses.append((school, name))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing: Normalization\n",
    "<img src=\"nlp_norm_0.png\">\n",
    "\n",
    "* Words with capitalization have low liklihood to influence the meaning of the statement\n",
    ">* Common to convert all words (and acronyms) to lowercase\n",
    "```python\n",
    "text = \"WoRds With&CapiTalization!'\n",
    "text.lower()\n",
    ">>> \"words with&capitalization!'\n",
    "```\n",
    "* Depending on context, punctuation wont change the meaning of the statement either, especially at a high level\n",
    ">* Replacing punctuations with a space instead of removal is helpful for eliminating the possiblity of words becomming concatenated\n",
    ">* Common regex values: `r\"[^a-zA-Z0-9]\"`\n",
    "```python\n",
    "import re\n",
    "re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    ">>> \"words with capitalization'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing: Tokenization\n",
    "Token: a symbol that holds a meaning\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "text = \"Dogs are the best\"\n",
    "text.lower().split()\n",
    ">>> [\"dogs\", \"are\", \"the\", \"best\"]\n",
    "```\n",
    "\n",
    "**USING NLTK: Natural Language Toolkit**\n",
    "\n",
    "NLTK can help us to tokenize words in a more robust way that captures other elements, such as the example below:\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Dr. Smith graduated from the University of Washington. He started Lux, an analytics firm.\"\n",
    "word_tokenize(text)\n",
    ">>> [\"Dr.\", \"Smith\", \"graduated\", \"from\", \"the\", \"University\", \"of\", \"Washington\", \".\", \"He\", \"started\", \"Lux\", \",\", \"an\", \"analytics\", \"firm\", \".\"]\n",
    "```\n",
    "\n",
    "And for sentences as well:\n",
    "```python\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Dr. Smith graduated from the University of Washington. He started Lux, an analytics firm.\"\n",
    "sent_tokenize(text)\n",
    ">>> [\"Dr. Smith graduated from the University of Washington.\", \"He started Lux, an analytics firm.\"]\n",
    "```\n",
    "\n",
    "* `nltk.tokenize` [package](http://www.nltk.org/api/nltk.tokenize.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing: Stop Words\n",
    "Stop Words: Words that are very common and do not add meanining to a sentence (in general)\n",
    ">* Examples: \"is\", \"are\", \"at\", \"the\", etc.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))\n",
    ">>> ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "```\n",
    "Removing stopwords with nltk:\n",
    "```python\n",
    "words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing: Parts of Speech\n",
    "Part-of-speech tagging using a predefined grammar like this is a simple, but limited, solution. It can be very tedious and error-prone for a large corpus of text, since you have to account for all possible sentence structures and tags!\n",
    "\n",
    "There are other more advanced forms of POS tagging that can learn sentence structures and tags from given data, including Hidden Markov Models (HMMs) and Recurrent Neural Networks (RNNs).\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "sentence = word_tokenize(\"I always lie down to tell a lie.\")\n",
    "pos_tag(sentence)\n",
    "sentence = word_tokenizer(\"I always lie down to tell a lie.\")\n",
    "pos_tag(sentence)\n",
    ">>> [('I', 'PRP'),\n",
    " ('always', 'RB'),\n",
    " ('lie', 'VBP'),\n",
    " ('down', 'RP'),\n",
    " ('to', 'TO'),\n",
    " ('tell', 'VB'),\n",
    " ('a', 'DT'),\n",
    " ('lie', 'NN'),\n",
    " ('.', '.')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing: Named Entity Recognition\n",
    "Named Entity: can be thought of as propernouns\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "text = \"Antonio joined Udacity Inc. in California.\"\n",
    "tree = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "print(tree)\n",
    "tree.draw()\n",
    ">>>\n",
    "(S\n",
    "  (PERSON Antonio/NNP)\n",
    "  joined/VBD\n",
    "  (ORGANIZATION Udacity/NNP Inc./NNP)\n",
    "  in/IN\n",
    "  (GPE California/NNP)\n",
    "  ./.)\n",
    "```\n",
    "<img src=\"nlp_tree_0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing: Stemming\n",
    "<img src=\"nlp_stem_0.png\">\n",
    "Reduce words to basic versions that hold only those most basic elements\n",
    "\n",
    "Stemming follows rules that can reduce some words to meaningless representations, such as {caches, caching, cache} to cach, but if all instances of the base word cache are reduced to cach, the meaning isn't lost at the macro sense.\n",
    "\n",
    "```python\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "```\n",
    "## Text Processing: Lemmatization\n",
    "<img src=\"nlp_lemma_0.png\">\n",
    "\n",
    "Lemmatization uses a dictionary to map variances of a word or sentiment back to its root\n",
    "\n",
    "The default lemmatizer in NLTK uses the WordNet database to reduce words to its lemmatized version\n",
    "```python\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemed = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "```\n",
    "\n",
    "Another option involves overriding the Part-Of-Speech parameter, which defaults to noun, with `v` for verb:\n",
    "```python\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w, pos='v') for w in lemmed]\n",
    "```\n",
    "Notice that `lemmed` in the second code block is initialized in the first code block. The lemmatization procedure has been chained together to account for multiple parts of speech!\n",
    "\n",
    "When choosing between Lemmatization or Stemming, stemming may be a less memory intensive operation to consider as it doesn't require a dictionary of predefined outcomes associated with an input.\n",
    "<img src=\"nlp_lem_stem_0.png\">\n",
    "\n",
    "It is common to apply both, lemmatization first, and then stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Prcoessing: Summary\n",
    "<img src=\"nlp_proc_sum_0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Feature Extraction\n",
    "\n",
    "Once our text has been converted to something usable, clean, and simplified, it may need to be transformed in a way that an algorithm can handle.\n",
    "\n",
    "Letters and numbers have symbolic representations in ASCII, but a letter cannot be meaniningfully compared to another letter or a number (in most cases). We are generally interested in the way these values will be compared not as letters and numbers though, but as combinations of them representing words - the computer has no way to make sense of these representations without some influence.\n",
    "\n",
    "We must extract some features from the texts to make them meaningful for the machine to interpret!\n",
    "\n",
    "[WordNet visualization tool](http://mateogianolio.com/wordnet-visualization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction: Bag of Words\n",
    "\n",
    "* Bag of words model: treats each document to be analyzed as an unordered \"bag\" of words\n",
    "* Document: Unit of text to be analyzed\n",
    "* Corpus: set of documents\n",
    "* Vocabulary: set of unique words/tokens in the corpus\n",
    "* Document-Term-Matrix: representation of tokens and frequencies per document in corpus\n",
    "<img src=\"nlp_dtm_0.png\">\n",
    "\n",
    "Measures of similarity:\n",
    "\n",
    "1. Dot-Product: $a\\cdot b=\\sum a_{0}b_{0}+a_{1}b_{1}+...+a_{n}b_{n}$\n",
    "2. Cosine-Similarity: $\\cos\\left(\\theta\\right)=\\frac{a\\cdot b}{\\lVert a \\rVert \\cdot \\lVert b \\rVert}$\n",
    ">* identical vectors = 1\n",
    ">* opposite vectors = -1\n",
    ">* no relation = 0\n",
    "\n",
    "<img src=\"nlp_meas_0.png\">\n",
    "\n",
    "### Feature Extraction: Term-Frequency Inverse-Document-Frequency (TF IDF)\n",
    "\n",
    "* TF IDF: representation of the number of tokens appearing in a document, inversely proportional to the number of documents per corpus\n",
    "\n",
    ">* A way to assign weights to words that signify their relevance in documents\n",
    "\n",
    "* **Term Frequency:** raw count of a term t in a document d, divided by the total number of terms in the document d\n",
    "\n",
    "* **Inverse Document Frequency:** logarithm of total number of documents in the collection D, divided by the number of documents d, in D, where the term t is present in d\n",
    "\n",
    "<img src=\"nlp_tfidf_0.png\">\n",
    "\n",
    "$$tfidf\\left(t = term,\\ d = document,\\ D = corpus\\right) = tf\\left(t,\\ d \\right) \\cdot idf\\left(t,\\ D\\right) $$\n",
    "$$tf\\left(t,\\ d\\right) = count\\left(t,\\ d\\right) \\div \\vert d \\vert$$\n",
    "$$idf\\left(t,\\ D\\right) = \\log\\left(\\vert D \\vert \\div \\vert \\left\\{ d \\in D : t \\in d \\right\\} \\vert \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
