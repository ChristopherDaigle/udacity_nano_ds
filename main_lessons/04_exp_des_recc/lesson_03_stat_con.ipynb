{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Considerations in Testing\n",
    "\n",
    "<!-- How do you deduce **causality**? *Run an experiment!*\n",
    "\n",
    "Covered in this lesson:\n",
    "1. [What is an expirement](#what_is)<br>\n",
    "\n",
    "> 1.1 [1.1 What types of studies are there](#study_types)<br>\n",
    "\n",
    "2. [What types of experiments are there](#types)<br>\n",
    "\n",
    "> 2.1 [Types of Sampling](#sampling)<br>\n",
    "\n",
    "3. [How are outcomes measured](#measured)<br>\n",
    "\n",
    "> 3.1: [Creating Metrics](#met_create)<br>\n",
    "\n",
    "> 3.2: [Controlling Variables](#cont_var)<br>\n",
    "\n",
    "> 3.3: [Checking Validity](#validity)<br>\n",
    "\n",
    "> 3.4: [Checking Bias](#bias)<br>\n",
    "\n",
    "> 3.5: [Ethics in Experimentation](#ethics)<br>\n",
    "\n",
    "4. [Experiment Design Plaining](#planning)<br>\n",
    "\n",
    "By the end of this section, you will know what is required to create experiments that effectively address your goals. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How much data will you need before you can judge the success of your experiment on solid grounds?\n",
    "* How many data points will I need to see the effect I am interested in\n",
    "> * Factors like the size of the effect that you want to see can have a major effect on how much data you need to collect and how long it will take before you get your results\n",
    "\n",
    "<img src=\"stat_cons_01.png\" width=\"500\">\n",
    "\n",
    "<img src=\"stat_cons_02.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: L2 Statistical Significance - Exercise\n",
    "\n",
    "This lesson assumes that you already know about basic inferential statistics. In particular, you should know how to perform a statistical test for the difference in means between two groups, and for comparing the mean of a single group against a reference value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we've collected data for a web-based experiment. In the experiment, we're testing the change in layout of a product information page to see if this affects the proportion of people who click on a button to go to the download page. This experiment has been designed to have a cookie-based diversion, and we record two things from each user: which page version they received, and whether or not they accessed the download page during the data recording period. (We aren't keeping track of any other factors in this example, such as number of pageviews, or time between accessing the page and making the download, that might be of further interest.)\n",
    "\n",
    "Your objective in this notebook is to perform a statistical test on both recorded metrics to see if there is a statistical difference between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.font_manager:Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats import proportion as proptests\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>condition</th>\n",
       "      <th>click</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   condition  click\n",
       "0          1      0\n",
       "1          0      0\n",
       "2          0      0\n",
       "3          1      1\n",
       "4          1      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "data = pd.read_csv('01_stat_considerations/data/statistical_significance_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset, the 'condition' column takes a 0 for the control group, and 1 for the experimental group. The 'click' column takes a values of 0 for no click, and 1 for a click.\n",
    "\n",
    "## Checking the Invariant Metric\n",
    "\n",
    "First of all, we should check that the number of visitors assigned to each group is similar. It's important to check the invariant metrics as a prerequisite so that our inferences on the evaluation metrics are founded on solid ground. If we find that the two groups are imbalanced on the invariant metric, then this will require us to look carefully at how the visitors were split so that any sources of bias are accounted for. It's possible that a statistically significant difference in an invariant metric will require us to revise random assignment procedures and re-do data collection.\n",
    "\n",
    "In this case, we want to do a two-sided hypothesis test on the proportion of visitors assigned to one of our conditions. Choosing the control or the experimental condition doesn't matter: you'll get the same result either way. Feel free to use whatever method you'd like: we'll highlight two main avenues below.\n",
    "\n",
    "If you want to take a simulation-based approach, you can simulate the number of visitors that would be assigned to each group for the number of total observations, assuming that we have an expected 50/50 split. Do this many times (200,000 repetitions should provide a good speed-variability balance in this case) and then see in how many simulated cases we get as extreme or more extreme a deviation from 50/50 that we actually observed. Don't forget that, since we have a two-sided test, an extreme case also includes values on the opposite side of 50/50. (e.g. Since simulated outcomes of .48 and lower are considered as being more extreme than an actual observation of 0.48, so too will simulated outcomes of .52 and higher.) The proportion of flagged simulation outcomes gives us a p-value on which to assess our observed proportion. We hope to see a larger p-value, insufficient evidence to reject the null hypothesis.\n",
    "\n",
    "If you want to take an analytic approach, you could use the exact binomial distribution to compute a p-value for the test. The more usual approach, however, is to use the normal distribution approximation. Recall that this is possible thanks to our large sample size and the central limit theorem. To get a precise p-value, you should also perform a \n",
    "continuity correction, either adding or subtracting 0.5 to the total count before computing the area underneath the curve. (e.g. If we had $\\frac{415}{850}$ assigned to the control group, then the normal approximation would take the area to the left of $\\frac{415 + 0.5}{850} = 0.489$ and to the right of $\\frac{435 - 0.5}{850} = 0.511$ .)\n",
    "\n",
    "You can check your results by completing the quiz and watching the video following the workspace. You could also try using multiple approaches and seeing if they come up with similar outcomes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of trials and number of 'successes'\n",
    "n_obs = data.shape[0]\n",
    "n_control = data.groupby('condition').size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5062175977346661\n",
      "0.6127039025537114\n"
     ]
    }
   ],
   "source": [
    "# Compute a z-score and p-value\n",
    "p = 0.5\n",
    "sd = np.sqrt(p * (1-p) * n_obs)\n",
    "\n",
    "z = ((n_control + 0.5) - p * n_obs) / sd\n",
    "\n",
    "print(z)\n",
    "print(2 * stats.norm.cdf(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of trials and number of 'successes'\n",
    "n_obs = data.shape[0]\n",
    "n_control = data.groupby('condition').size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.614335\n",
      "p-value for the test on the invariant metric (number of visitors assigned to each group): 0.614335\n"
     ]
    }
   ],
   "source": [
    "# # simulate outcomes under null, compare to observed outcome\n",
    "p = 0.5\n",
    "n_trials = 200_000\n",
    "\n",
    "samples = np.random.binomial(n_obs, p, n_trials)\n",
    "\n",
    "# print(np.logical_or(samples <= n_control, samples >= (n_obs - n_control)).mean())\n",
    "invar_p = np.logical_or(samples <= n_control, samples >= (n_obs - n_control)).mean()\n",
    "print(\"p-value for the test on the invariant metric (number of visitors assigned to each group): {}\".format(invar_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Evaluation Metric\n",
    "\n",
    "After performing our checks on the invariant metric, we can move on to performing a hypothesis test on the evaluation metric: the click-through rate. In this case, we want to see that the experimental group has a significantly larger click-through rate than the control group, a one-tailed test.\n",
    "\n",
    "The simulation approach for this metric isn't too different from the approach for the invariant metric. You'll need the overall click-through rate as the common proportion to draw simulated values from for each group. You may also want to perform more simulations since there's higher variance for this test.\n",
    "\n",
    "There's a few analytic approaches possible here, but you'll probably make use of the normal approximation again in these cases. In addition to the pooled click-through rate, you'll need a pooled standard deviation in order to compute a z-score. While there is a continuity correction possible in this case as well, it's much more conservative than the p-value that a simulation will usually imply. Computing the z-score and resulting p-value without a continuity correction should be closer to the simulation's outcomes, though slightly more optimistic about there being a statistical difference between groups.\n",
    "\n",
    "As with the previous question, you'll find a quiz and video following the workspace for you to check your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "condition\n",
       "0    0.079430\n",
       "1    0.112205\n",
       "Name: click, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_click = data.groupby('condition').mean()['click']\n",
    "p_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03277498917523293"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Difference in average click rate between groups\n",
    "p_click[1] - p_click[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of trials and overall 'success' rate under null\n",
    "n_control = data.groupby('condition').size()[0]\n",
    "n_exper = data.groupby('condition').size()[1]\n",
    "p_null = data['click'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value for the test on the evaluation metric (difference in click-through rates across groups): 0.039\n"
     ]
    }
   ],
   "source": [
    "# compute standard error, z-score, and p-value\n",
    "se_p = np.sqrt(p_null * (1-p_null) * (1/n_control + 1/n_exper))\n",
    "\n",
    "z = (p_click[1] - p_click[0]) / se_p\n",
    "# print(z)\n",
    "eval_p=1-stats.norm.cdf(z)\n",
    "print(\"p-value for the test on the evaluation metric (difference in click-through rates across groups): {:.3f}\".format(eval_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of trials and overall 'success' rate under null\n",
    "n_control = data.groupby('condition').size()[0]\n",
    "n_exper = data.groupby('condition').size()[1]\n",
    "p_null = data['click'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03986\n"
     ]
    }
   ],
   "source": [
    "# simulate outcomes under null, compare to observed outcome\n",
    "n_trials = 200_000\n",
    "\n",
    "ctrl_clicks = np.random.binomial(n_control, p_null, n_trials)\n",
    "exp_clicks = np.random.binomial(n_exper, p_null, n_trials)\n",
    "samples = exp_clicks / n_exper - ctrl_clicks / n_control\n",
    "\n",
    "print((samples >= (p_click[1] - p_click[0])).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Practical Significance\n",
    "\n",
    "**Practical Significance:** *Level of observed change required to deploy a tested experimental manipulation\n",
    "\n",
    "<center><img src=\"stat_cons_06.png\" width=\"500\"></center>\n",
    "\n",
    "\n",
    "Even if an experiment result shows a statistically significant difference in an evaluation metric between control and experimental groups, that does not necessarily mean that the experiment was a success. If there are any costs associated with deploying a change, those costs might outweigh the benefits expected based on the experiment results. **Practical significance** refers to the level of effect that you need to observe in order for the experiment to be called a true success and implemented in truth. Not all experiments imply a practical significance boundary, but it's an important factor in the interpretation of outcomes where it is relevant.\n",
    "\n",
    "If you consider the confidence interval for an evaluation metric statistic against the null baseline and practical significance bound, there are a few cases that can come about.\n",
    "\n",
    "### Confidence interval is fully in practical significance region\n",
    "(Below, $m_{0}$indicates the null statistic value, $d_{min}$ the practical significance bound, and the blue line the confidence interval for the observed statistic. We assume that we're looking for a positive change, ignoring the negative equivalent for $d_{min}$)\n",
    "\n",
    "<center><img src=\"stat_cons_03.png\" width=\"500\"></center>\n",
    "\n",
    "If the confidence interval for the statistic does not include the null or the practical significance level, then the experimental manipulation can be concluded to have a statistically and practically significant effect. It is clearest in this case that the manipulation should be implemented as a success.\n",
    "\n",
    "### Confidence interval completely excludes any part of practical significance region\n",
    "\n",
    "<center><img src=\"stat_cons_04.png\" width=\"500\"></center>\n",
    "\n",
    "If the confidence interval does not include any values that would be considered practically significant, this is a clear case for us to not implement the experimental change. This includes the case where the metric is statistically significant, but whose interval does not extend past the practical significance bounds. With such a low chance of practical significance being achieved on the metric, we should be wary of implementing the change.\n",
    "\n",
    "### Confidence interval includes points both inside and outside practical significance bounds\n",
    "\n",
    "<center><img src=\"stat_cons_05.png\" width=\"500\"></center>\n",
    "\n",
    "This leaves the trickiest cases to consider, where the confidence interval straddles the practical significance bound. In each of these cases, there is an uncertain possibility of practical significance being achieved. In an ideal world, you would be able to collect more data to reduce our uncertainty, reducing the scenario to one of the previous cases. Outside of this, you'll need to consider the risks carefully in order to make a recommendation on whether or not to follow through with a tested change. Your analysis might also reveal subsets of the population or aspects of the manipulation that **do** work, in order to refine further studies or experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Experiment Size\n",
    "\n",
    "<center><img src=\"stat_cons_07.png\" width=\"500\"></center>\n",
    "\n",
    "We can use the knowledge of our desired practical significance boundary to plan out our experiment. By knowing how many observations we need in order to detect our desired effect to our desired level of reliability, we can see how long we would need to run our experiment and whether or not it is feasible.\n",
    "\n",
    "Let's use the example from the video, where we have a baseline click-through rate of 10% and want to see a manipulation increase this baseline to 12%. How many observations would we need in each group in order to detect this change with power $1-\\beta = .80$ (i.e. detect the 2% absolute increase 80% of the time), at a Type I error rate of $\\alpha = .05$?\n",
    "\n",
    "The curves on these two plots represent the difference in sample means given 1,000 observations in each of the control groups and experimental groups, with the top being no effect from the treatment and the bottom being the desired outcome from the treatment, a 2% absolute increase:\n",
    "<center><img src=\"stat_cons_08.png\" width=\"500\"></center>\n",
    "\n",
    "The vertical line at 0.02 indicates a type-one error rate of 5% (because 95% of the data lies to the left of it). 0.02 is the critical value. Matching that critical value on the desired result shows a type one error rate of 44%, and type two error rate on the left of the line of 56%:\n",
    "<center><img src=\"stat_cons_09.png\" width=\"500\"></center>\n",
    "\n",
    "Increasing the number of data points will narrow both curves, increasing the statistical power. If we want a statistical power of 0.8, or an 80% chance of rejecting the null, assuming a 12% TRUE click-through rate (or treatment effect), then we need at least 2,863 observations in each the control and test group. We can achieve this over 12 days if we get about 500 people a day. $\\frac{500}{2} \\cdot x \\approx 2863 \\rightarrow \\frac{2863}{250} = x = 11.45 \\approx 12$\n",
    "<center><img src=\"stat_cons_09.png\" width=\"500\"></center>\n",
    "\n",
    "After computing the number of observations needed for an experiment to reliably detect a specified level of experimental effect (i.e. statistical power), we need to divide by the expected number of observations per day in order to get a minimum experiment length. We want to make sure that an experiment can be completed in a reasonable time frame so that if we do have a successful effect, it can be deployed as soon as possible and resources can be freed up to run new experiments. What a 'reasonable time frame' means will depend on how important a change will be, but if the length of time is beyond a month or two, that's probably a sign that it's too long.\n",
    "\n",
    "There are a few ways that an experiment's duration can be reduced. We could, of course, change our statistical parameters. Accepting higher Type I or Type II error rates will reduce the number of observations needed. So too will increasing the effect size: it's much easier to detect larger changes.\n",
    "\n",
    "Another option is to change the unit of diversion. A 'wider' unit of diversion will result in more observations being generated. For example, you could consider moving from a cookie-based diversion in a web-based experiment to an event-based diversion like pageviews. The tradeoff is that event-based diversion could create inconsistent website experiences for users who visit the site multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Size - Exercise\n",
    "\n",
    "We can use the knowledge of our desired practical significance boundary to plan out our experiment. By knowing how many observations we need in order to detect our desired effect to our desired level of reliability, we can see how long we would need to run our experiment and whether or not it is feasible.\n",
    "\n",
    "Let's use the example from the video, where we have a baseline click-through rate of 10% and want to see a manipulation increase this baseline to 12%. How many observations would we need in each group in order to detect this change with power $1-\\beta = .80$ (i.e. detect the 2% absolute increase 80% of the time), at a Type I error rate of $\\alpha = .05$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Trial and Error\n",
    "\n",
    "One way we could solve this is through trial and error. Every sample size will have a level of power associated with it; testing multiple sample sizes will gradually allow us to narrow down the minimum sample size required to obtain our desired power level. This isn't a particularly efficient method, but it can provide an intuition for how experiment sizing works.\n",
    "\n",
    "Fill in the `power()` function below following these steps:\n",
    "\n",
    "1. Under the null hypothesis, we should have a critical value for which the Type I error rate is at our desired alpha level.\n",
    "  - `se_null`: Compute the standard deviation for the difference in proportions under the null hypothesis for our two groups. The base probability is given by `p_null`. Remember that the variance of the difference distribution is the sum of the variances for the individual distributions, and that _each_ group is assigned `n` observations.\n",
    "  - `null_dist`: To assist in re-use, this should be a [scipy norm object](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html). Specify the center and standard deviation of the normal distribution using the \"loc\" and \"scale\" arguments, respectively.\n",
    "  - `p_crit`: Compute the critical value of the distribution that would cause us to reject the null hypothesis. One of the methods of the `null_dist` object will help you obtain this value (passing in some function of our desired error rate `alpha`).\n",
    "2. The power is the proportion of the distribution under the alternative hypothesis that is past that previously-obtained critical value.\n",
    "  - `se_alt`: Now it's time to make computations in the other direction. This will be standard deviation of differences under the desired detectable difference. Note that the individual distributions will have different variances now: one with `p_null` probability of success, and the other with `p_alt` probability of success.\n",
    "  - `alt_dist`: This will be a scipy norm object like above. Be careful of the \"loc\" argument in this one. The way the `power` function is set up, it expects `p_alt` to be greater than `p_null`, for a positive difference.\n",
    "  - `beta`: Beta is the probability of a Type-II error, or the probability of failing to reject the null for a particular non-null state. That means you should make use of `alt_dist` and `p_crit` here!\n",
    "\n",
    "The second half of the function has already been completed for you, which creates a visualization of the distribution of differences for the null case and for the desired detectable difference. Use the cells that follow to run the function and observe the visualizations, and to test your code against a few assertion statements. Check the following page if you need help coming up with the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ud",
   "language": "python",
   "name": "venv_ud"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
